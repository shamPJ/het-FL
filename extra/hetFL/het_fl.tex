\documentclass{article}
\usepackage{graphicx} % Required for inserting images
\usepackage{amssymb}
\usepackage{listings}
\usepackage{amsmath} 
\usepackage{algorithm}
\usepackage{algpseudocode}

\newcommand{\graph}{\mathcal{G}}

\title{hetFL}
\author{shamsiiat.abdurakhmanova }
\date{\today}

\begin{document}

\maketitle

\section{Numerical experiments}

\subsection{Datasets}

\subsubsection{Synthetic Dataset}

Experiments were performed on a synthetic dataset whose empirical graph $\mathcal {G}$ is partitioned into 3 equal-sized clusters $\mathcal{P} = \{\mathcal{C}^{(1)}, \mathcal{C}^{(2)}, \mathcal{C}^{(3)}\}$, with $|\mathcal{C}^{(1)}|=|\mathcal{C}^{(2)}|=|\mathcal{C}^{(3)}|$. We denote the cluster assignment of node $i \in \mathcal{V}$ by ${c}^{(i)} \in \{1,2,3\}$. 

For experiments where graph connectivity is known, the edges in $\mathcal {G}$  are generated via realizations of independent binary random variables ${b}_{i,{i}^{'}} \in \{0,1\}$. These random variables are indexed by pairs $i,{i}^{'}$ of nodes that are connected by an edge $\{i,{i}^{'}\} \in \mathcal{E}$ if and only if ${b}_{i,{i}^{'}}=1$. \\
Two nodes in the same cluster are connected with probability $Prob\{{b}_{i,i'}=1\} :={p}_{in}$ if nodes $i,i'$ belong to the same cluster. In contrast, $Prob\{{b}_{i,{i}^{'}}=1\} :={p}_{out}$ if nodes $i,{i}^{'}$ belong to different clusters. Every edge in  $\mathcal {G}$ has the same weight, ${A}_{e}=1$ for all $e \in \mathcal{E}$.

Each node $i \in \mathcal {V}$ of the empirical graph $\mathcal {G}$ holds a local dataset $\mathcal {D}^{(i)}$ of the form $\mathcal {D}^{(i)} := \{ (\mathbf{x}^{(i,1)}, {y}^{(i,1)}), ..., (\mathbf{x}^{(i,{m}_{i})}, {y}^{(i,{m}_{i})}) \}$. Thus, dataset $\mathcal {D}^{(i)}$ consist of ${m}_{i}$ data points, each characterized by a feature vector $\mathbf{x}^{(i,r)} \in \mathbb{R}^{d}$ and scalar label ${y}^{(i,r)}$, for $r=1,...,{m}_{i}$. The feature vectors $\mathbf{x}^{(i,r)} \sim \mathcal{N}(\mathbf{0},\mathbf{I}_{d \times d})$, are drawn i.i.d. from a standard multivariate normal distribution. 

The labels of the data points are generated by a noisy linear model
\begin{equation}
{y}^{(i,r)} = (\mathbf{w}^{(i)})^T\mathbf{x}^{(i,r)} + {\varepsilon}^{(i,r)}
\end{equation}

The noise ${\varepsilon}^{(i,r)} \sim \mathcal{N}(0, 1)$, for $i \in \mathcal{V}$ and $r=1,..,{m}_{i}$, are i.i.d. realizations of a normal distribution. The true underlying vector $\mathbf{\overline{w}}^{(i)} \sim \mathcal{N}(0,1)$ is drawn from a standard normal distribution and is the same for nodes from the same cluster, i.e. $\mathbf{\overline{w}}^{(i)}=\mathbf{\overline{w}}^{({i'})}$ if ${c}^{(i)}={c}^{({i'})}$.

Datasets are divided into training and validation subsets by using resampling with replacement. The size of the validation subset is ${m}^{(val)}_{i}=100$. 

\subsubsection{Shared Dataset}

Dataset $\mathcal{D}^{(test)}$, which predictions are shared across all nodes was formed as follows:
the feature, weight and noise vectors are drawn i.i.d. from a standard normal distribution and labels are generated by a noisy linear model. The size of the dataset is $m'=100$. 

\subsection{Experiments}

\subsubsection{Synthetic Dataset, linreg model, graph is known}

In these experiments empirical graph $\mathcal{G}$ consist of $N=15$ nodes partitioned into three clusters ($|\mathcal{C}^{(i)}|=5$). Two nodes in the same cluster are connected with probability ${p}_{in}=0.8$ if nodes $i,{i}^{'}$ belong to the same cluster and ${p}_{out}=0.2$ if nodes $i,{i}^{'}$ belong to different clusters. 

Each node $i \in \mathcal {V}$ of the empirical graph $\mathcal {G}$ holds a local dataset $\mathcal{D}^{(i)}$ consisting of ${m}_{i}$ data points, each characterized by a feature vector $\mathbf{x}^{(i,r)} \in \mathbb{R}^{d}$ and scalar label ${y}^{(i,r)}$, for $r=1,...,{m}_{i}$, where $d=10$.
The sample size of the shared dataset $\mathcal{D}^{(test)}$ is $m'=100$.

To learn the local parameters $\mathbf{w}^{(i)}$, we use Algorithm X with local loss 

\begin{equation}
{L}_{(i)}({h}^{(i)}) = \frac{1}{{m}_{i}} \sum_{r=1}^{{m}_{i}}\left({y}^{(i,r)} - {h}^{(i)}(\mathbf{x}^{(i,r)}) \right)^2
\end{equation}

and  regularizer 

\begin{equation}
\frac{\lambda}{2m'} \sum_{{i}^{'} \in {\mathcal V}} {A}_{i,{i}^{'}} \sum_{r=1}^{m'} \left({h}^{(i)}(\mathbf{x}^{(r)}) - {h}^{({i}^{'})}(\mathbf{x}^{(r)}) \right)^2
\end{equation}

For the experiment we use linear model implemented with pytorch (no bias, optimizer SGD learning rate 0.01). We try different local dataset sizes $m_i =$ \{2, 3, 5, 10, 20\} and regularization strength $\lambda =$ \{0, 0.1, 0.5\}. Tried ratio $|\mathcal{C}^{(i)}|m_i / d = \{1, 1.5, 2.5, 5, 10\}$ or $d / m_i = \{5, 3.3, 2, 1, 0.5\}$ 

As stopping criterion in Algorithm 2, we use a fixed number of R = 3000 iterations. 
For pytorch models one iteration is equivalent to one gradient step.

Below is the plot of mean estimation error (mean over 10 repetitions of the experiment for each pair of $\{\lambda, m_i \}$).

\begin{equation}
\frac{1}{N} \sum_{i=1}^{N} ||\mathbf{\overline{w}}^{(i)} - \mathbf{\widehat{w}}^{(i)}||^2_2
\end{equation}

On each repetition new local $\mathcal{D}^{(i)}$ and shared $\mathcal{D}^{(test)}$ datasets were generated. The shaded region is $\pm$ one standard deviation.  

\includegraphics[width=10cm]{linreg_with_G.png}

\newpage
\begin{algorithm}[htbp]
	\caption{Least-Square Regression (Adjacency matrix is known)}
	\label{alg_X_param}
	{\bf Input}: empirical graph $\graph$ with edge weights $A_{ij}$; 
	local loss $L_{(i)}{(\cdot)}$; shared dataset $D^{(test)} = \{\mathbf{x}^{(1)}, ..., \mathbf{x}^{(m')}\}$; GTV parameter $\lambda$; \\
	{\bf Initialize}: $k:=0; \widehat{h}_{0}^{(i)} \!\equiv\!$ for all nodes $i \in \mathcal{V}$.
	\begin{algorithmic}[1]
		\While{stopping criterion is not met do}
		\For{all nodes $i \in \mathcal{V}$ in parallel}
		\State share predictions $\{\widehat{h}_{k}^{(i)}(\mathbf{x})\}_{\mathbf{x} \in \mathcal{D}^(test)}$, 
		with neighbours $i' \in \mathcal{N}^{(i)}$ 
		\State update local hypothesis $\widehat{h}_{k}^{(i)}$ by
            \begin{multline*}
            \widehat{h}_{k+1}^{(i)} \in  \arg \min_{h^{(i)} \in \mathcal{H}^{(i)}} 
            \biggl[ \frac{1}{m_i}  \sum_{r=1}^{m_i} \biggl( y^{(i,r)} - h^{(i)}(\mathbf{x}^{(i,r)})  \biggr)^2 + \\
            \frac{\lambda}{m'} \sum_{i' \in \mathcal{N}^{(i)}}A_{i,i'}\sum_{r=1}^{m'} \biggl( h^{(i)}(\mathbf{x}^{(r)}) - \widehat{h}_{k}^{(i')}(\mathbf{x}^{(r)}) \biggr)^2 \biggr]
            \end{multline*}
		\EndFor
		\State $k := k+1$
		\EndWhile
			  \Ensure local $\widehat{h}^{(i)} := \widehat{h}_{k+1}^{(i)}$ for all nodes $i \in \mathcal{V}$
	\end{algorithmic}
\end{algorithm}

\newpage
\subsubsection{Synthetic Dataset, linreg model, graph is not known}

\begin{algorithm}[htbp]
	\caption{Least-Square Regression (Adjacency matrix is not known)}
	\label{alg_X_param}
	{\bf Input}: empirical graph $\graph$; 
	local loss $L_{(i)}{(\cdot)}$; shared dataset $D^{(test)} = \{\mathbf{x}^{(1)}, ..., \mathbf{x}^{(m')}\}$; GTV parameter $\lambda$; \\
	{\bf Initialize}: $k:=0; A:=0; \widehat{h}_{0}^{(i)} \!\equiv\!$ for all nodes $i \in \mathcal{V}$.
	\begin{algorithmic}[1]
		\While{stopping criterion is not met do}
		\For{all nodes $i \in \mathcal{V}$ in parallel}
		\State share predictions $\{\widehat{h}_{k}^{(i)}(\mathbf{x})\}_{\mathbf{x} \in \mathcal{D}^(test)}$, 
		with neighbours $i' \in \mathcal{N}^{(i)}$ 
		\State update local hypothesis $\widehat{h}_{k}^{(i)}$ by
            \begin{multline*}
            \widehat{h}_{k+1}^{(i)} \in  \arg \min_{h^{(i)} \in \mathcal{H}^{(i)}} 
            \biggl[ \frac{1}{m_i}  \sum_{r=1}^{m_i} \biggl( y^{(i,r)} - h^{(i)}(\mathbf{x}^{(i,r)})  \biggr)^2 + \\
            \frac{\lambda}{m'} \sum_{i' \in \mathcal{N}^{(i)}}A_{i,i'}\sum_{r=1}^{m'} \biggl( h^{(i)}(\mathbf{x}^{(r)}) - \widehat{h}_{k}^{(i')}(\mathbf{x}^{(r)}) \biggr)^2 \biggr]
            \end{multline*}
		\EndFor
		\State $k := k+1$
        \For{all nodes $i \in \mathcal{V}$ in parallel}
        \State find  p-neighbours for the node by selecting nodes with p-smallest values of
             \begin{align}
                \frac{1}{m'} \sum_{r=1}^{m'} \biggl( h^{(i)}(\mathbf{x}^{(r)}) - \widehat{h}_{k}^{(i')}(\mathbf{x}^{(r)}) \biggr)^2 
             \end{align}
        
        
        \EndFor
		\EndWhile
			  \Ensure local $\widehat{h}^{(i)} := \widehat{h}_{k+1}^{(i)}$ for all nodes $i \in \mathcal{V}$
	\end{algorithmic}
\end{algorithm}

\newpage
number of neighbors  p=3.
\vspace{5mm}

\includegraphics[width=10cm]{linreg_nneib_3.png}

\vspace{5mm}
number of neighbors p=5
\vspace{5mm}

\includegraphics[width=10cm]{linreg_no_G_nneib_5.png}

\subsubsection{ TODO Synthetic Dataset, models of mixed type, graph is not known}

\end{document}
